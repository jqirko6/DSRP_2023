# Day 8 Notes

## Load Libraries

```{r}
#install.packages("tidymodels")
#install.packages("reshape2")
library(parsnip)
library(rsample)
library(yardstick)
library(tidyr)
library(janitor)
library(dplyr)
library(reshape2)
library(ggplot2)
```

```{r}
install.packages("ranger")
install.packages("xgboost")
```

## Ifelse()

```{r}
ifelse()
case_when() #dplyr function
# saving plots
```

```{r}
x <- 7
x <- c(1, 3, 5, 7 ,9)
ifelse(x < 5, "small number", "big number")
```

```{r}
head(iris)
mean(iris$Petal.Width)
iris_new <- iris

## Add categorical column
iris_new <- mutate(iris_new,
                   petal_size = ifelse(Petal.Width > 1, "big", "small"))
iris_new
```

## Case_when()

```{r}
iris_new <- mutate(iris_new,
                   petal_size = case_when(
                     Petal.Width < 1 ~ "small",
                     Petal.Width < 2 ~ "medium",
                     Petal.Width >= 2 ~ "big"
                   ))

iris_new
```

# Saving Plots

```{r}
ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width)) +
  geom_point()
ggsave("plots/scatterPlot.png")
```

# PCA

```{r}
pcas <- prcomp(iris_num, scale. = T)
summary(pcas)
pcas$rotation

pcas$rotation^2 

## get the x values of PCAs and make it a data frame
pca_vals <- as.data.frame(pcas$x)
pca_vals$Species <- iris$Species

ggplot(pca_vals, aes(PC1, PC2, color = Species)) +
  geom_point() +
  theme_minimal()

## Revie PCA
```

# Supervised Learning

## Classification

Assign observations in specific categories. Classification - Will it be Hot or Cold tomorrow? Also groups observations into "classes"

## Regression

How much will it rain tomorrow? Predicts a numeric value.

# Supervised Machine Learning Models

## Collect data

```{r}
head(iris)
```

## Clean and Process Data

Get rid of NAs. Only use na.omit when you have specifically selected for the variables you want to include in the model

```{r}
noNAs <- na.omit(starwars)

noNAs <- filter(starwars, !is.na(mass), is.na(height))

## Replace with means
replaceWithMeans <- mutate(starwars,
                           mass = ifelse(is.na(mass),
                                         mean(mass), 
                                         mass))
```

| Encoding categories as factors or integers
| If categorical variable is a character, make it a factor

```{r}
intSpecies <- mutate(starwars, 
                     species = as.integer(as.factor(species)))
```

| If categorical variable is already a factor, make it an integer

```{r}
irisAllNumeric <- mutate(iris,
                         Var2 = as.integer(Species))
```

## Visualize Data

| Make a PCA
| Calculate correlations

```{r}
cor(irisAllNumeric)

irisCors <- cor(irisAllNumeric) |>
  cor() |>
  melt() |>
  as.data.frame()

ggplot(irisCors, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile()


```

## Preform Feature Selection

| Choose which variables you want to classify or predict
| Choose which variables you want to use as features in your model
| For iris data...
| Classify on speices (classification) & predict on sepal.lenght

## 5 Separate data into test/train

| Should have twice as much test than train data
| choose 70-80% of data to train on

```{r}
library(rsample)
```

```{r}
set.seed(71723)
```

Put 75% of data into the training set

```{r}
# Regression datatset splits
reg_split <- initial_split(irisAllNumeric, prop = .75)
reg_train <- training(reg_split)
reg_test <- testing(reg_split)
```

```{r}
# Classification dataset splits (use iris instead of irisAllNumeric)
class_split <- reg_split <- initial_split(iris, prop = .75)
class_train <- training(class_split)
class_test <- testing(class_split)

class_test
```

## Choose Suitable Model

### Linear Regression

-   Simple, limited to only linear relationships

-   Easy to understand/interpret

-   Only uses numeric data

y = m1x1 + m2x2 + b

`function_name()` is `linear_reg()`

`"engine_name"` is `"glm"` or `"lm"`

```{r}
lm_fit <- linear_reg() |>
  set_engine("lm") |>
  set_mode("regression") |>
  fit(Sepal.Length ~ Petal.Length + Petal.Width + Species + Sepal.Width,
      data = reg_train)

## Sepal.Length = 2.3 + Petal.Length*0.7967 + Petal.Width* -0.4067 + Species*-0.3312  + Sepal.Width* 0.5501

lm_fit$fit
summary(lm_fit$fit)
```

### Logistic Regression

| Fits data in to a more flexible function
| Used when y is a binomial categorical variable
| x values can be numeric or categorical

```{r}

binary_test_data <- filter(class_test, Species %in% c("setosa", "versicolor"))
biinary_train_data <- filter(class_train, Species %in% c("setosa", "versicolor"))\


log_fit <- logistic_reg() |>
  set_engine("glm") |>
  set_mode("classification") |>
  fit(Species ~ Petal.Width + Petal.Length + ., data = class_train)

log_fit$fit
summary(log_fit$fit)



```

### Boosted Trees & Random Forest

| Uses a decision tree structure instead of a formula
| Used to predict categorical or numeric variable
| Less interpret-able and more computationally intensive

```{r}
#regression
boost_reg_fit <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("regression") |>
  fit(Sepal.Length ~ ., data = reg_train)

boost_fit$fit$evaluation_log

#classification
boost_class_fit <- boost_tree() |>
  set_engine("xgboost") |>
  set_mode("classification") |>
  fit(Species ~ ., data = class_train)
```

#### Random Forest

```{r}
forest_reg_fit <- rand_forest() |>
  set_engine("ranger") |>
  set_mode("regression") |>
  fit(Sepal.Length ~ ., data = reg_train)

forest_reg_fit$fit

forest_class_fit <- rand_forest() |>
  set_engine("ranger") |>
  set_mode("classification") |>
  fit(Species ~ ., data = class_train)

forest_class_fit$fit
```

### Evaluate Model Performance on Test Set

Calculate errors for regression

`lm_fit` , `boost_reg_fit` , `forest_reg_fit`

```{r}
reg_results <- reg_test

reg_results$lm_pred <- predict(lm_fit, reg_test)$.pred
reg_results$boost_pred <- predict(boost_reg_fit, reg_test)$.pred
reg_results$forest_pred <- predict(forest_reg_fit, reg_test)$.pred

yardstick::mae(reg_results, Sepal.Length, lm_pred)
yardstick::mae(reg_results, Sepal.Length, boost_pred)
yardstick::mae(reg_results, Sepal.Length, forest_pred)
```

```{r}
#install.packages("MLmetrics")
library(MLmetrics)
```
